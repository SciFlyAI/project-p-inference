{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание onnx из .pt файла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Установка зависимостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установите зависимости для YoLo\n",
    "\n",
    "Сначала для torch.cpu:\n",
    "https://github.com/ValV/yolov7/blob/master/requirements.torch.cpu.txt\n",
    "\n",
    "Остальные:\n",
    "https://github.com/ValV/yolov7/blob/master/requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка самой библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting yolov7\n",
      "  Cloning https://github.com/ValV/yolov7.git (to revision package) to c:\\users\\user\\appdata\\local\\temp\\pip-install-2sefd0ej\\yolov7_595d265ee7b448c692765954c75299a2\n",
      "  Resolved https://github.com/ValV/yolov7.git to commit f7d72f0b0d41f996d8a21268c05bc29f23d97196\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: yolov7\n",
      "  Building wheel for yolov7 (setup.py): started\n",
      "  Building wheel for yolov7 (setup.py): finished with status 'done'\n",
      "  Created wheel for yolov7: filename=yolov7-0.1.0-py3-none-any.whl size=132407 sha256=e52a59ffc7b59a9877e11ff710aa794430c635f56de3e3b1d28044376cd40b28\n",
      "  Stored in directory: C:\\Users\\user\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-crt6n5ce\\wheels\\db\\d1\\c7\\e5d94140ce224fd3346cf691d55dee88bb5d22ba6670d68fd2\n",
      "Successfully built yolov7\n",
      "Installing collected packages: yolov7\n",
      "Successfully installed yolov7-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/ValV/yolov7.git 'C:\\Users\\user\\AppData\\Local\\Temp\\pip-install-2sefd0ej\\yolov7_595d265ee7b448c692765954c75299a2'\n",
      "  Running command git checkout -b package --track origin/package\n",
      "  branch 'package' set up to track 'origin/package'.\n",
      "  Switched to a new branch 'package'\n"
     ]
    }
   ],
   "source": [
    "%pip install -U git+https://github.com/ValV/yolov7.git@package#egg=yolov7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка библиотек для взаимодействия с onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'nvidia-pyindex'\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: \"'onnx\"\n"
     ]
    }
   ],
   "source": [
    "%pip install 'nvidia-pyindex'\n",
    "%pip install 'onnx>=1.9.0' 'onnx-simplifier>=0.3.6' 'onnx-graphsurgeon' 'protobuf~=3.19.6'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Код"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from os import path as osp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.utils.data\n",
    "import yaml\n",
    "from torch.cuda import amp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yolov7.models.common import Conv\n",
    "from yolov7.models.experimental import attempt_load, End2End\n",
    "from yolov7.utils.activations import Hardswish, SiLU\n",
    "from yolov7.utils.general import set_logging, check_img_size\n",
    "from yolov7.utils.torch_utils import select_device\n",
    "from yolov7.utils.add_nms import RegisterNMS\n",
    "\n",
    "\n",
    "from os import makedirs\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция для импорта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def export(\n",
    "    weights: str,  # weights path\n",
    "    img_size: int = [640, 640],  # image size\n",
    "    batch_size: int = 1,  # batch size\n",
    "    dynamic: bool = False,  # dynamic ONNX axes\n",
    "    dynamic_batch: bool = False,  # dynamic batch onnx for tensorrt and onnx-runtime (disables dynamic axes)\n",
    "    grid: bool = False,  # export Detect() layer grid\n",
    "    end2end: bool = False,  # export end2end onnx (disables dynamic axes)\n",
    "    max_wh: int = None,  # None for tensorrt nms, int value for onnx-runtime nms\n",
    "    topk_all: int = 100,  # topk objects for every image\n",
    "    iou_thres: float = 0.45,  # iou threshold for NMS\n",
    "    conf_thres: float = 0.25,  # conf threshold for NMS\n",
    "    device: str = 'cpu',  # cuda device, i.e. 0 or 0,1,2,3 or cpu\n",
    "    simplify: bool = False,  # simplify onnx model\n",
    "    include_nms: bool = False,  # export end2end onnx\n",
    "    fp16: bool = False,  # CoreML FP16 half-precision export\n",
    "    int8: bool = False  # CoreML INT8 quantization\n",
    "):\n",
    "    img_size *= 2 if len(img_size) == 1 else 1  # expand\n",
    "    dynamic = dynamic and not end2end\n",
    "    dynamic = False if dynamic_batch else dynamic\n",
    "    set_logging()\n",
    "    t = time.time()\n",
    "\n",
    "    # Load PyTorch model\n",
    "    device = select_device(device)\n",
    "    model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "    labels = model.names\n",
    "\n",
    "    # Checks\n",
    "    gs = int(max(model.stride))  # grid size (max stride)\n",
    "    img_size = [check_img_size(x, gs) for x in img_size]  # verify img_size are gs-multiples\n",
    "\n",
    "    # Input\n",
    "    img = torch.zeros(batch_size, 3, *img_size).to(device)  # image size(1, 3, 320, 192) iDetection\n",
    "\n",
    "    # Update model\n",
    "    for k, m in model.named_modules():\n",
    "        m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatibility\n",
    "        if isinstance(m, Conv):  # assign export-friendly activations\n",
    "            if isinstance(m.act, nn.Hardswish):\n",
    "                m.act = Hardswish()\n",
    "            elif isinstance(m.act, nn.SiLU):\n",
    "                m.act = SiLU()\n",
    "        # elif isinstance(m, models.yolo.Detect):\n",
    "        #     m.forward = m.forward_export  # assign forward (optional)\n",
    "    model.model[-1].export = not grid  # set Detect() layer grid export\n",
    "    y = model(img)  # dry run\n",
    "    if include_nms:\n",
    "        model.model[-1].include_nms = True\n",
    "        y = None\n",
    "\n",
    "    # TorchScript export\n",
    "    try:\n",
    "        print('\\nStarting TorchScript export with torch %s...' % torch.__version__)\n",
    "        f = weights.replace('.pt', '.torchscript.pt')  # filename\n",
    "        ts = torch.jit.trace(model, img, strict=False)\n",
    "        ts.save(f)\n",
    "        print('TorchScript export success, saved as %s' % f)\n",
    "    except Exception as e:\n",
    "        print('TorchScript export failure: %s' % e)\n",
    "\n",
    "    # CoreML export\n",
    "    try:\n",
    "        import coremltools as ct\n",
    "\n",
    "        print('\\nStarting CoreML export with coremltools %s...' % ct.__version__)\n",
    "        # convert model from torchscript and apply pixel scaling as per detect.py\n",
    "        ct_model = ct.convert(ts, inputs=[ct.ImageType('image', shape=img.shape, scale=1 / 255.0, bias=[0, 0, 0])])\n",
    "        bits, mode = (8, 'kmeans_lut') if int8 else (16, 'linear') if fp16 else (32, None)\n",
    "        if bits < 32:\n",
    "            if sys.platform.lower() == 'darwin':  # quantization only supported on macOS\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # suppress numpy==1.20 float warning\n",
    "                    ct_model = ct.models.neural_network.quantization_utils.quantize_weights(ct_model, bits, mode)\n",
    "            else:\n",
    "                print('quantization only supported on macOS, skipping...')\n",
    "\n",
    "        f = weights.replace('.pt', '.mlmodel')  # filename\n",
    "        ct_model.save(f)\n",
    "        print('CoreML export success, saved as %s' % f)\n",
    "    except Exception as e:\n",
    "        print('CoreML export failure: %s' % e)\n",
    "\n",
    "    # TorchScript-Lite export\n",
    "    try:\n",
    "        print('\\nStarting TorchScript-Lite export with torch %s...' % torch.__version__)\n",
    "        f = weights.replace('.pt', '.torchscript.ptl')  # filename\n",
    "        tsl = torch.jit.trace(model, img, strict=False)\n",
    "        tsl = optimize_for_mobile(tsl)\n",
    "        tsl._save_for_lite_interpreter(f)\n",
    "        print('TorchScript-Lite export success, saved as %s' % f)\n",
    "    except Exception as e:\n",
    "        print('TorchScript-Lite export failure: %s' % e)\n",
    "\n",
    "    # ONNX export\n",
    "    try:\n",
    "        import onnx\n",
    "\n",
    "        print('\\nStarting ONNX export with onnx %s...' % onnx.__version__)\n",
    "        f = weights.replace('.pt', '.onnx')  # filename\n",
    "        model.eval()\n",
    "        output_names = ['classes', 'boxes'] if y is None else ['output']\n",
    "        dynamic_axes = None\n",
    "        if dynamic:\n",
    "            dynamic_axes = {\n",
    "                'images': {0: 'batch', 2: 'height', 3: 'width'},  # size(1, 3, 640, 640)\n",
    "                'output': {0: 'batch', 2: 'y', 3: 'x'}\n",
    "            }\n",
    "        if dynamic_batch:\n",
    "            batch_size = 'batch'\n",
    "            dynamic_axes = {\n",
    "                'images': {\n",
    "                    0: 'batch',\n",
    "                },\n",
    "            }\n",
    "            if end2end and max_wh is None:\n",
    "                # TensorRT end2end\n",
    "                output_axes = {\n",
    "                    'num_dets': {0: 'batch'},\n",
    "                    'det_boxes': {0: 'batch'},\n",
    "                    'det_scores': {0: 'batch'},\n",
    "                    'det_classes': {0: 'batch'},\n",
    "                }\n",
    "            else:\n",
    "                # Onnxruntime\n",
    "                output_axes = {\n",
    "                    'output': {0: 'batch'},\n",
    "                }\n",
    "            dynamic_axes.update(output_axes)\n",
    "        if grid:\n",
    "            if end2end:\n",
    "                # End2end Detect() layer grid export\n",
    "                print('\\nStarting export end2end onnx model for %s...' % 'TensorRT' if max_wh is None else 'onnxruntime')\n",
    "                model = End2End(model, topk_all, iou_thres, conf_thres, max_wh, device, len(labels))\n",
    "                if end2end and max_wh is None:\n",
    "                    # TensorRT end2end\n",
    "                    output_names = ['num_dets', 'det_boxes', 'det_scores', 'det_classes']\n",
    "                    shapes = [batch_size, 1, batch_size, topk_all, 4,\n",
    "                              batch_size, topk_all, batch_size, topk_all]\n",
    "                else:\n",
    "                    # Onnxruntime end2end\n",
    "                    output_names = ['output']\n",
    "            else:\n",
    "                # Basic Detect() layer grid export\n",
    "                model.model[-1].concat = True\n",
    "\n",
    "        torch.onnx.export(\n",
    "            model, img, f, verbose=False, opset_version=12,\n",
    "            input_names=['images'],\n",
    "            output_names=output_names,\n",
    "            dynamic_axes=dynamic_axes\n",
    "        )\n",
    "\n",
    "        # Checks\n",
    "        onnx_model = onnx.load(f)  # load onnx model\n",
    "        onnx.checker.check_model(onnx_model)  # check onnx model\n",
    "\n",
    "        if end2end and max_wh is None:\n",
    "            # TensorRT end2end\n",
    "            for i in onnx_model.graph.output:\n",
    "                for j in i.type.tensor_type.shape.dim:\n",
    "                    j.dim_param = str(shapes.pop(0))\n",
    "\n",
    "        # print(onnx.helper.printable_graph(onnx_model.graph))  # print a human readable model\n",
    "\n",
    "        # # Metadata\n",
    "        # d = {'stride': int(max(model.stride))}\n",
    "        # for k, v in d.items():\n",
    "        #     meta = onnx_model.metadata_props.add()\n",
    "        #     meta.key, meta.value = k, str(v)\n",
    "        # onnx.save(onnx_model, f)\n",
    "\n",
    "        if simplify:\n",
    "            try:\n",
    "                import onnxsim\n",
    "\n",
    "                print('\\nStarting to simplify ONNX...')\n",
    "                onnx_model, check = onnxsim.simplify(onnx_model)\n",
    "                assert check, 'assert check failed'\n",
    "            except Exception as e:\n",
    "                print(f'Simplifier failure: {e}')\n",
    "\n",
    "        # print(onnx.helper.printable_graph(onnx_model.graph))  # print a human readable model\n",
    "        onnx.save(onnx_model, f)\n",
    "        print('ONNX export success, saved as %s' % f)\n",
    "\n",
    "        if include_nms:\n",
    "            print('Registering NMS plugin for ONNX...')\n",
    "            mo = RegisterNMS(f)\n",
    "            # if cuda is available -- TRT\n",
    "            # else something without TRT \n",
    "            mo.register_nms()\n",
    "            mo.save(f)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise\n",
    "        print('ONNX export failure: %s' % e)\n",
    "\n",
    "    # Finish\n",
    "    print('\\nExport complete (%.2fs). Visualize with https://github.com/lutzroeder/netron.' % (time.time() - t))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код, который запускает сборку onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR  2024-4-9 torch 2.2.2+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting best.pt...\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\project-p\\lib\\site-packages\\torch\\functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3550.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 200 layers, 6017434 parameters, 0 gradients, 13.1 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting TorchScript export with torch 2.2.2+cpu...\n",
      "TorchScript export success, saved as .\\weights\\best.torchscript.pt\n",
      "CoreML export failure: No module named 'coremltools'\n",
      "\n",
      "Starting TorchScript-Lite export with torch 2.2.2+cpu...\n",
      "TorchScript-Lite export failure: name 'optimize_for_mobile' is not defined\n",
      "\n",
      "Starting ONNX export with onnx 1.12.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\envs\\project-p\\lib\\site-packages\\yolov7\\models\\yolo.py:590: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if augment:\n",
      "c:\\Users\\user\\miniconda3\\envs\\project-p\\lib\\site-packages\\yolov7\\models\\yolo.py:622: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if profile:\n",
      "c:\\Users\\user\\miniconda3\\envs\\project-p\\lib\\site-packages\\yolov7\\models\\yolo.py:637: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if profile:\n",
      "YOLOR  2024-4-9 torch 2.2.2+cpu CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting to simplify ONNX...\n",
      "ONNX export success, saved as .\\weights\\best.onnx\n",
      "\n",
      "Export complete (75.34s). Visualize with https://github.com/lutzroeder/netron.\n",
      "\n",
      "\n",
      "\n",
      "Exporting last.pt...\n",
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 200 layers, 6017434 parameters, 0 gradients, 13.1 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting TorchScript export with torch 2.2.2+cpu...\n",
      "TorchScript export success, saved as .\\weights\\last.torchscript.pt\n",
      "CoreML export failure: No module named 'coremltools'\n",
      "\n",
      "Starting TorchScript-Lite export with torch 2.2.2+cpu...\n",
      "TorchScript-Lite export failure: name 'optimize_for_mobile' is not defined\n",
      "\n",
      "Starting ONNX export with onnx 1.12.0...\n",
      "\n",
      "Starting to simplify ONNX...\n",
      "ONNX export success, saved as .\\weights\\last.onnx\n",
      "\n",
      "Export complete (82.39s). Visualize with https://github.com/lutzroeder/netron.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config: ...\n",
    "\n",
    "\n",
    "opt = Config()\n",
    "opt.save_dir = '.'\n",
    "\n",
    "\n",
    "path_export_source = osp.join(opt.save_dir, 'weights')\n",
    "# path_export_target = osp.join(opt.save_dir, 'models')\n",
    "\n",
    "# makedirs(path_export_target, exist_ok=True)\n",
    "\n",
    "size_input = [3840, 2176]  # [3840, 2160] (4K) + multiple of 32\n",
    "for path_weights in glob(osp.join(path_export_source, '????.pt')):\n",
    "    if 'init.pt' in path_weights:\n",
    "        continue\n",
    "    print(f\"Exporting {osp.basename(path_weights)}...\")\n",
    "    export(\n",
    "        weights=path_weights,\n",
    "        img_size=size_input,  # opt.img_size,\n",
    "        dynamic=True,\n",
    "        # dynamic_batch=True,\n",
    "        # end2end=True,\n",
    "        # max_wh=size_input,\n",
    "        simplify=True,\n",
    "        include_nms=False\n",
    "    )\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
